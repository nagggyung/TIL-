### Study from Deep lizard - lecture: Machine learning & deep learning fundamentals

1. Machine learning: learn from data 
2. Deep learning is a sub-field of machine learning that uses algorithms inspired by the structure and function of the brain's neural networks.

### Artificial Neural Networks(ANNs)
* ANNs are builts using what we call neurons.
* Neurons in an ANN are organized into what we call layers
* Layers within an ANN(all but the input and output layers) are called hidden layer.
* If an ANN has more than one hidden layer, the ANN is said to be a deep ANN.

In summary, deep learning uses ANNs that have multiple hidden layer.

![2021-08-06 (3)](https://user-images.githubusercontent.com/74478432/128385211-04e6327f-9600-42a3-9caa-992d82827f7b.png)

3. An artificial neural network is a computing system that is comprised of a collection of connected units called neurons that are organized into what we call layers.

Nodes are organized into what we call layers.
At the highest level, there are three types of layers in every ANN:
* Input layer
* Hidden layer
* Output layer

Different layers perform different kinds of transformations on their inputs.
Data flows through the network starting at the input layer and moving through the hidden layers until the output layer is reached.
This is known as a forward pass through the network.
Layers positioned between the input and output layers are know as hidden layer. 

In keras, we can build what we called a sequential model. Keras defines a sequential model as a sequential stack of linear layers. 

```c
from keras.models import Sequential 
from keras.layers import Dense, Activation

layers =[
  Dense(units=3, input_shape=(2, ), activation ='relu'),
  Dense(units=2, activation = 'softmax')
]

model = Sequential(layers)
```

![2021-08-06 (4)](https://user-images.githubusercontent.com/74478432/128386482-fa1bf890-7160-4de5-bf19-6a5a5ee788b2.png)

4. Dense layers are also known as 'fully connected layers'

Different layers perform different transformations on their input, and some layers are better suited for some tasks than others.
For example, a convolutional layer is usually ised in models that are doing work with image data.


### Layer weights

Each connection between two nodes has an associated weight, which is just a number.
Each weight represents the strength of the connection between two nodes.
When the network receives an input at a given node in the input layer, this input is passed to the next node via a connection, and the input will be multiplied by the weight assigned to that connection.

For each node in the second layer, a weighted sum is then computed with each of incoming connections. This sum is then passed to an activation function, which performs some type of transformation of the given sum.

For example, an activation function may transform the sum to be a number between zero and one. The actual transformation will vary depending on which activation function is used. 

* node output = activation(weighted sum of inputs)

The number of nodes in the output layer depends on the number of possible output or prediction classes we have.

5. What is an action function?
In an artificial neural network, an activation function is a function that maps a node's inputs to its corresponding output.

### what do activation function do?
* Relu Activation Function: Relu, which is short for rectified linear unit, transforms the input to the maximum of either 0 or the input itself.
  relu(x) = max(0,x)
  * so, if the input is less than or equal to 0, then relu will output 0.
  * if the input is greater than 0, relu will then just output the given input. 

6. What is training?
When we train a mode, we're basically trying to solve an optimization problem. We're trying to optimize the weights within the model. 
Our task is to find the weights that most accurately map our input data to the correct output class.
This mapping is what the network must learn.

### Optimization Algorithm
The weights are optimized using what we call an optimization algorithm.
We also use the term optimizer to refer to the chosen algorithm.
The most widely known optimizer is called stochastic gradient descent, or more simply, SGD.

The objective of SGD is to minimize some given function that we call a loss function. So, SGD updates the model's weights in such a way as to make this loss function as close to its minimum value as possible.

### Loss function
One common loss function is mean squared error(MSE), but there are several loss functions that we could use in its place. It's our job to decide which loss function to use. 

7. An epoch refers to a single pass of the entire dataset to the network during training.

### What does it mean to learn?
When the model is initialized, the network weights are set to arbitrary values. At the end of the network, the model will provide the output for a given input. 
Once the output is obtained, the loss(or the error) can be computed for that specific output by looking at what model predicted versus the true label.

The loss computation depends on the chosen loss function.

### Gradient of the loss function
After the loss is calculated, the gradient of this loss function is computed with respect to each of the weights within the network.
* gradient of the loss function = loss/weight

At this point, we've calculated the loss of a single output, and we calculate the gradient of that loss with respect to our single chosen weight.
Once we have the value of the gradient, this values will then be multiplied by something called a learning rate. A learning rate is a small number usually ranging between 0.01 and 0.0001 but the actual value can vary. So, any value we get for the gradient is going to become relatively small once we multiply it by the learning rate. )

The calculation is done using a technique called back-propagation.

Once we have the value for the gradient of the loss function, we can use this value to update the model's weight. The gradient tells us which direction will move the loss towards the minimum, and our task is to move in a direction that lowers the loss and steps closer to this minimum value. 


