##Study from Deep lizard - lecture: Machine learning & deep learning fundamentals

### chap1. Machine learning: learn from data 
### chap2. Deep learning is a sub-field of machine learning that uses algorithms inspired by the structure and function of the brain's neural networks.

### Artificial Neural Networks(ANNs)
* ANNs are builts using what we call neurons.
* Neurons in an ANN are organized into what we call layers
* Layers within an ANN(all but the input and output layers) are called hidden layer.
* If an ANN has more than one hidden layer, the ANN is said to be a deep ANN.

In summary, deep learning uses ANNs that have multiple hidden layer.

![2021-08-06 (3)](https://user-images.githubusercontent.com/74478432/128385211-04e6327f-9600-42a3-9caa-992d82827f7b.png)

### chap3. 

An artificial neural network is a computing system that is comprised of a collection of connected units called neurons that are organized into what we call layers.

Nodes are organized into what we call layers.
At the highest level, there are three types of layers in every ANN:
* Input layer
* Hidden layer
* Output layer

Different layers perform different kinds of transformations on their inputs.
Data flows through the network starting at the input layer and moving through the hidden layers until the output layer is reached.
This is known as a forward pass through the network.
Layers positioned between the input and output layers are know as hidden layer. 

In keras, we can build what we called a sequential model. Keras defines a sequential model as a sequential stack of linear layers. 

```c
from keras.models import Sequential 
from keras.layers import Dense, Activation

layers =[
  Dense(units=3, input_shape=(2, ), activation ='relu'),
  Dense(units=2, activation = 'softmax')
]

model = Sequential(layers)
```

![2021-08-06 (4)](https://user-images.githubusercontent.com/74478432/128386482-fa1bf890-7160-4de5-bf19-6a5a5ee788b2.png)

### chap4. Dense layers are also known as 'fully connected layers'

Different layers perform different transformations on their input, and some layers are better suited for some tasks than others.
For example, a convolutional layer is usually ised in models that are doing work with image data.


### Layer weights

Each connection between two nodes has an associated weight, which is just a number.
Each weight represents the strength of the connection between two nodes.
When the network receives an input at a given node in the input layer, this input is passed to the next node via a connection, and the input will be multiplied by the weight assigned to that connection.

For each node in the second layer, a weighted sum is then computed with each of incoming connections. This sum is then passed to an activation function, which performs some type of transformation of the given sum.

For example, an activation function may transform the sum to be a number between zero and one. The actual transformation will vary depending on which activation function is used. 

* node output = activation(weighted sum of inputs)

The number of nodes in the output layer depends on the number of possible output or prediction classes we have.

### chap5. What is an action function?

In an artificial neural network, an activation function is a function that maps a node's inputs to its corresponding output.

### what do activation function do?
* Relu Activation Function: Relu, which is short for rectified linear unit, transforms the input to the maximum of either 0 or the input itself.
  relu(x) = max(0,x)
  * so, if the input is less than or equal to 0, then relu will output 0.
  * if the input is greater than 0, relu will then just output the given input. 

### chap6. What is training?

When we train a mode, we're basically trying to solve an optimization problem. We're trying to optimize the weights within the model. 
Our task is to find the weights that most accurately map our input data to the correct output class.
This mapping is what the network must learn.

### Optimization Algorithm
The weights are optimized using what we call an optimization algorithm.
We also use the term optimizer to refer to the chosen algorithm.
The most widely known optimizer is called stochastic gradient descent, or more simply, SGD.

The objective of SGD is to minimize some given function that we call a loss function. So, SGD updates the model's weights in such a way as to make this loss function as close to its minimum value as possible.

### Loss function
One common loss function is mean squared error(MSE), but there are several loss functions that we could use in its place. It's our job to decide which loss function to use. 

### chap7.

An epoch refers to a single pass of the entire dataset to the network during training.

### What does it mean to learn?
When the model is initialized, the network weights are set to arbitrary values. At the end of the network, the model will provide the output for a given input. 
Once the output is obtained, the loss(or the error) can be computed for that specific output by looking at what model predicted versus the true label.

The loss computation depends on the chosen loss function.

### Gradient of the loss function
After the loss is calculated, the gradient of this loss function is computed with respect to each of the weights within the network.
* gradient of the loss function = loss/weight

At this point, we've calculated the loss of a single output, and we calculate the gradient of that loss with respect to our single chosen weight.
Once we have the value of the gradient, this values will then be multiplied by something called a learning rate. A learning rate is a small number usually ranging between 0.01 and 0.0001 but the actual value can vary. So, any value we get for the gradient is going to become relatively small once we multiply it by the learning rate. )

The calculation is done using a technique called back-propagation.

Once we have the value for the gradient of the loss function, we can use this value to update the model's weight. The gradient tells us which direction will move the loss towards the minimum, and our task is to move in a direction that lowers the loss and steps closer to this minimum value. 

### Learning rate
We then multipy the gradient value by something called a learning rate.
A learning rate is a small number usually between 0.01 and 0.0001, but the actual value can vary.
The learning rate tells us how large of a step we should take in the direction of the minimum.

### Updating the weights
We multiply the gradient with the learning rate, and we subtrack this product from the weight, which will give us the new updated value for this weight.
* new weight = old weight - (learning rate * gradient)

So, now imagine all these weights being iteratively updated with each epoch.
The weights are going to be incrementally getting closer and closer to their optimized values while SGD works to minimize the loss function.

### The model is learning
This updating of the weight is essentially what we mean when we say that model is learning. 
It's learning what values to assign to each weight based on how those incremental changes are affecting the loss function. 

As the weights chage, the network is getting smarter in terms of accurately mapping input to the correct output.

Before we can train our model, we must compile it like so:

```c
model.compile(
      optimizer = Adam(learning_rate = 0.0001),
      loss = 'Sparse_categorical_crossentropy',
      metrics = ['accuracy']
)
```


Finally, we fit our model to the data. Fitiing the model to the data means to train the model on the data.

```c
model. fit(
      x = Scaled_train_samples,
      y = train_labels,
      batch_size = 10,
      epochs = 20,
      shuffle = True,
      verbose = 2
)
```
* Scaled_train_samples is a 'numpy array' consisting of the training samples. 
* train_labels is a 'numpy array' consisting of the corresponding labels for the training samples. 
* batch_size = 10 specifies how many training samples should be sent to the model at once.
* epochs = 20 means that the complete training set(all of the samples) will be passed to the model a total of 20 times.
* verbose = 2 indicates how much logging we will see as the model trains.

What you will notice is that the loss is going down and the accuracy is going up as the epochs progress. 


### chap8. 

At the end of each epoch during the training process, the loss will be calculated using the network's output predictions and the true labels for the respective input. 

Suppose our model is classifying images of cats and dogs, and assume that the label for cat is '0' and the label for dog is '1'.
* Cat: 0
* Dog: 1

Now suppose we pass an image of a cat to the model, and the provided output is 0.25.
In this case, the difference between the model's prediction and the true label is 0.25-0.00 = 0.25.

The difference is also called the error.
* error = 0.25 - 0.00 = 0.25

This process is performed for every output. For each epoch, trhe error is accumulated across all the individual outputs.

### Mean Squared Error(MSE)

For a single sample, with MSE, we first calculate the difference (the error) between the provided output prediction and the label.
We then squar this error,
* MSE(input) = (output-label)(output-label)

The implementation of what we actually do with each of the errors will be dependent upon the algorithm of the given loss function we're using. 

Given that the objective of SGD is to minimize the loss, we want to see our loss decrease as we run more epochs.
